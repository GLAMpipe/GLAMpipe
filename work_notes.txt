
What is GLAMpipe?
High Level Data Processing and Storage API - with a GUI


TESTING
worker-farm -> huge performance boost!
- makes communication a little bit more complex. -> Generic process counter is needed, child processes are aware only on they part


TODO:
- node run register /stop functionality / force
- mark fields that node produces, also out.setter
- remove node dir on node remove

- WEB export core
- File downloads
- CLD core
- user auth

- custom node functionality
- forked node functionality
- allow labeling of collections

- shibboleth
- devmode


DONE:
- PDF core
- script node
- allow labeling of nodes
- facet and search queries
- facet view
- add "views" section to UI
- finalize socket.io communication (messages must include project and node uuids) OK
- csv core OK
- node dirs

TO BE FIXED:
- re-generate schema should refresh keys
- facet UI should warn about '|' in data values

FIXED:
- OK node settings are not remembered 




API:
/api/v2/collections/[COLLECTION_ID]/fields
- returns all fields of the collection
- 

search
/api/v2/collections/[COLLECTION_ID]/docs
parameters:
- limit 
- skip
- keys -> keys included
- nokeys -> keys not included
- mongoquery -> query object that is passed to mongo "mongoquery={'title': 'My document'}"
- "title=kissa|koira"


Nodes and cores

Nodes are synchronous code. No file writing or web requests. In order to write file or make a web request the node can use core modules.

Node gives options to core module and the result is given back to node.

Currently available core modules:
- web
- (pdf)
- (csv)


CHANGES:
- nodes can be labeled -> /nodes/labels/dspace_thesesis_query/run
- custom nodes can be made -> every file in node must be editable -> PUT /nodes/[UUID]/scripts/run.js
- custom node can be saved as user node -> where to store?
- schema checker is run after every process node -> if there are new fields, then attach them to latest node run. This way we can get fields set by "out.setter"
- possibly option to disable schem checker
- nodes and config can be loaded on the fly (no need to restart container)
- up-to-date check for data -> for example, rerun of source node invalidates all flollowing processing and export nodes.
- automatic node ordening -> if node is dependent of the output of the other node, then the second node is show earlien in the node list.
- python export
- view nodes are now separated from export nodes
- master-data collection -> immutable collection that can be used in project like it was a normal collection

Core communication


* things that are set in GP-node
core.options
- options object for core module (core request)
- the format depends on core module (for example, options object for web-core options is the options of request module)

core.skip
- tell core to skip this request
- if skip is non-empty, it is passed to a next round as core.error
core.skip = "no file present"
if(core.error)
 out.value = GP.error + core.error


* Things that are set by core
core.data
- data that is coming from core module (core response)

core.error
- is set when core can not fulfill the request (file is missing, network error etc.)
  OR
  if core.skip is non-empty


OPTIONS
Options are node type specific values stored in the database. These are used mostly for URLs for nodes (like DSpace nodes)

SCHEMA
Every collection has their own schema in gp__schemas. Schema keeps track of fields in the collection. 
source:
- when a source node is run, the createSchema is run after every import
- this results the "original fields" list

process and export:
- if node uses out.setter, then fields in the setter are added to schema in every round.
- if node has setings.out_value, then this is added to the the schema when node is created
